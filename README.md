1. Project Subject and SummaryInternet information overload makes finding specific data difficult. This project develops an NLP-based text summarization application to quickly condense long texts, helping researchers and students save time during literature reviews. The project features a custom UI where users input text and receive a summary. Future updates may include support for direct file uploads.
2. Purpose and GoalThe primary goal is time efficiency. By providing short summaries of long articles or academic papers, users can instantly determine if a source is relevant. The project also aims to provide a simple, user-friendly interface.
3. Software UsedPython: Core programming language.VS Code: Primary development environment.PyQt5: Used for building the Graphical User Interface (GUI).NLTK: For natural language processing tasks.BeautifulSoup: For scraping text data from web sources (Wikipedia)
Implementation Steps & ResultsSetup: Installed PyQt5, NLTK, and BeautifulSoup.Data Acquisition: Built a function to fetch text from Wikipedia URLs, removing reference numbers and cleaning the data.Preprocessing: Tokenized sentences and removed punctuation to create a word frequency dictionary.Scoring Logic: Calculated "weighted frequency" for words. Sentences were then scored based on the sum of these weights.Summarization: Extracted the top $n$ highest-scoring sentences (defaulting to 7) to form the summary.Storage: Summaries are automatically saved to .txt files.Interface: Designed with Qt Designer. It offers two modes: Wikipedia Mode (summarize via URL) and Manual Mode (summarize pasted text).
Summary of the SummaryThis project is a Python-based desktop application that uses extractive summarization (word frequency scoring) to condense Wikipedia articles or user-provided text. It features a custom PyQt5 interface and saves all results as text files to assist in rapid literature research.
